{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2024.0.0\" \"nncf>=2.9.0\"\n",
    "%pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics==8.2.24\" onnx opencv-python tqdm --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "from pathlib import Path\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "from notebook_utils import download_file, VideoPlayer\n",
    "\n",
    "# Download a test sample\n",
    "IMAGE_PATH = Path(\"./data/coco_bike.jpg\")\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\",\n",
    "    filename=IMAGE_PATH.name,\n",
    "    directory=IMAGE_PATH.parent,\n",
    ")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "SEG_MODEL_NAME = \"yolov8n-seg\"\n",
    "\n",
    "seg_model = YOLO(models_dir / f\"{SEG_MODEL_NAME}.pt\")\n",
    "label_map = seg_model.model.names\n",
    "\n",
    "res = seg_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance segmentation model\n",
    "seg_model_path = models_dir / f\"{SEG_MODEL_NAME}_openvino_model/{SEG_MODEL_NAME}.xml\"\n",
    "if not seg_model_path.exists():\n",
    "    seg_model.export(format=\"openvino\", dynamic=True, half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "seg_ov_model = core.read_model(seg_model_path)\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    seg_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "seg_compiled_model = core.compile_model(seg_ov_model, device.value, ov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def infer(*args):\n",
    "    result = seg_compiled_model(args)\n",
    "    return torch.from_numpy(result[0]), torch.from_numpy(result[1])\n",
    "\n",
    "\n",
    "seg_model.predictor.inference = infer\n",
    "seg_model.predictor.model.pt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = seg_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "from ultralytics.data.utils import DATASETS_DIR\n",
    "\n",
    "\n",
    "DATA_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "LABELS_URL = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/v8.1.0/ultralytics/cfg/datasets/coco.yaml\"\n",
    "\n",
    "OUT_DIR = DATASETS_DIR\n",
    "\n",
    "DATA_PATH = OUT_DIR / \"val2017.zip\"\n",
    "LABELS_PATH = OUT_DIR / \"coco2017labels-segments.zip\"\n",
    "CFG_PATH = OUT_DIR / \"coco.yaml\"\n",
    "\n",
    "download_file(DATA_URL, DATA_PATH.name, DATA_PATH.parent)\n",
    "download_file(LABELS_URL, LABELS_PATH.name, LABELS_PATH.parent)\n",
    "download_file(CFG_URL, CFG_PATH.name, CFG_PATH.parent)\n",
    "\n",
    "if not (OUT_DIR / \"coco/labels\").exists():\n",
    "    with ZipFile(LABELS_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR)\n",
    "    with ZipFile(DATA_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR / \"coco/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: ov.Model,\n",
    "    core: ov.Core,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    validator,\n",
    "    num_samples: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model accuracy validation function. Runs model validation on dataset and returns metrics\n",
    "    Parameters:\n",
    "        model (Model): OpenVINO model\n",
    "        data_loader (torch.utils.data.DataLoader): dataset loader\n",
    "        validator: instance of validator class\n",
    "        num_samples (int, *optional*, None): validate model only on specified number samples, if provided\n",
    "    Returns:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "    \"\"\"\n",
    "    validator.seen = 0\n",
    "    validator.jdict = []\n",
    "    validator.stats = dict(tp_m=[], tp=[], conf=[], pred_cls=[], target_cls=[])\n",
    "    validator.batch_i = 1\n",
    "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
    "    model.reshape({0: [1, 3, -1, -1]})\n",
    "    num_outputs = len(model.outputs)\n",
    "    compiled_model = core.compile_model(model)\n",
    "    for batch_i, batch in enumerate(tqdm(data_loader, total=num_samples)):\n",
    "        if num_samples is not None and batch_i == num_samples:\n",
    "            break\n",
    "        batch = validator.preprocess(batch)\n",
    "        results = compiled_model(batch[\"img\"])\n",
    "        if num_outputs == 1:\n",
    "            preds = torch.from_numpy(results[compiled_model.output(0)])\n",
    "        else:\n",
    "            preds = [\n",
    "                torch.from_numpy(results[compiled_model.output(0)]),\n",
    "                torch.from_numpy(results[compiled_model.output(1)]),\n",
    "            ]\n",
    "        preds = validator.postprocess(preds)\n",
    "        validator.update_metrics(preds, batch)\n",
    "    stats = validator.get_stats()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_stats(stats: np.ndarray, total_images: int, total_objects: int):\n",
    "    \"\"\"\n",
    "    Helper function for printing accuracy statistic\n",
    "    Parameters:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "        total_images (int) -  number of evaluated images\n",
    "        total objects (int)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Boxes:\")\n",
    "    mp, mr, map50, mean_ap = (\n",
    "        stats[\"metrics/precision(B)\"],\n",
    "        stats[\"metrics/recall(B)\"],\n",
    "        stats[\"metrics/mAP50(B)\"],\n",
    "        stats[\"metrics/mAP50-95(B)\"],\n",
    "    )\n",
    "    # Print results\n",
    "    print(\"    Best mean average:\")\n",
    "    s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "        \"Class\",\n",
    "        \"Images\",\n",
    "        \"Labels\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"mAP@.5\",\n",
    "        \"mAP@.5:.95\",\n",
    "    )\n",
    "    print(s)\n",
    "    pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "    print(pf % (\"all\", total_images, total_objects, mp, mr, map50, mean_ap))\n",
    "    if \"metrics/precision(M)\" in stats:\n",
    "        s_mp, s_mr, s_map50, s_mean_ap = (\n",
    "            stats[\"metrics/precision(M)\"],\n",
    "            stats[\"metrics/recall(M)\"],\n",
    "            stats[\"metrics/mAP50(M)\"],\n",
    "            stats[\"metrics/mAP50-95(M)\"],\n",
    "        )\n",
    "        # Print results\n",
    "        print(\"    Macro average mean:\")\n",
    "        s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "            \"Class\",\n",
    "            \"Images\",\n",
    "            \"Labels\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"mAP@.5\",\n",
    "            \"mAP@.5:.95\",\n",
    "        )\n",
    "        print(s)\n",
    "        pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "        print(pf % (\"all\", total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "from ultralytics.utils import ops\n",
    "\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "args.data = str(CFG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_validator = seg_model.task_map[seg_model.task][\"validator\"](args=args)\n",
    "seg_validator.data = check_det_dataset(args.data)\n",
    "seg_validator.stride = 32\n",
    "seg_data_loader = seg_validator.get_dataloader(OUT_DIR / \"coco/\", 1)\n",
    "\n",
    "seg_validator.is_coco = True\n",
    "seg_validator.class_map = coco80_to_coco91_class()\n",
    "seg_validator.names = seg_model.model.names\n",
    "seg_validator.metrics.names = seg_validator.names\n",
    "seg_validator.nc = seg_model.model.model[-1].nc\n",
    "seg_validator.nm = 32\n",
    "seg_validator.process = ops.process_mask\n",
    "seg_validator.plot_masks = []\n",
    "NUM_TEST_SAMPLES = 300\n",
    "fp_seg_stats = test(seg_ov_model, core, seg_data_loader, seg_validator, num_samples=NUM_TEST_SAMPLES)\n",
    "print_stats(fp_seg_stats, seg_validator.seen, seg_validator.nt_per_class.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "int8_model_seg_path = models_dir / f\"{SEG_MODEL_NAME}_openvino_int8_model/{SEG_MODEL_NAME}.xml\"\n",
    "\n",
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch skip_kernel_extension module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "\n",
    "import nncf\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def transform_fn(data_item:Dict):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "       data_item: Dict with data item produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: Input data for quantization\n",
    "    \"\"\"\n",
    "    input_tensor = seg_validator.preprocess(data_item)['img'].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "quantization_dataset = nncf.Dataset(seg_data_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ignored_scope = nncf.IgnoredScope(\n",
    "    names=[\n",
    "        \"__module.model.22.cv3.0.0.conv/aten::_convolution/Convolution\",  # in the post-processing subgraph\n",
    "            \"__module.model.22.proto.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv4.0.0.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.16.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv2.0.0.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.6.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv3.1.1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.21.cv2.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.21.m.0.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22/aten::add/Add_6\",\n",
    "            \"__module.model.22/aten::sub/Subtract\",\n",
    "            \"__module.model.7.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.12.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.4.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv2.2.1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv2.0.1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv4.2.1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.dfl.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv3.2.2/aten::_convolution/Convolution\",\n",
    "            \"__module.model.22.cv3.0.2/aten::_convolution/Convolution\",\n",
    "            \"__module.model.15.cv1.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.5.conv/aten::_convolution/Convolution\",\n",
    "            \"__module.model.0.conv/aten::_convolution/Convolution\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Segmentation model\n",
    "quantized_seg_model = nncf.quantize(\n",
    "    seg_ov_model,\n",
    "    quantization_dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    ignored_scope=ignored_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "print(f\"Quantized segmentation model will be saved to {int8_model_seg_path}\")\n",
    "ov.save_model(quantized_seg_model, str(int8_model_seg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    quantized_seg_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "\n",
    "quantized_seg_compiled_model = core.compile_model(quantized_seg_model, device.value, ov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "\n",
    "def infer(*args):\n",
    "    result = quantized_seg_compiled_model(args)\n",
    "    return torch.from_numpy(result[0]), torch.from_numpy(result[1])\n",
    "\n",
    "seg_model.predictor.inference = infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "res = seg_model(IMAGE_PATH)\n",
    "display(Image.fromarray(res[0].plot()[:, :, ::-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
